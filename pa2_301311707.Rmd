--- 
title: Programming Assignment 2 
author: <Branton Li - 301311707> 
output: 
  html_document:
    mathjax: default 
--- 
```{r include=FALSE} 
#libraries are loaded here
#library("ggplot2")
library("tree")
library("plyr")
library("dplyr")

```

## Task 1 

```{r} 
#your code for task 1 comes here 
titanic <- read.csv(file="titanic3.csv")
titanic <- titanic[-1310,]
train_size <- floor(0.8*nrow(titanic))
set.seed(1)
tmp <- sample(1:nrow(titanic), size=train_size)
train <- titanic[tmp,]
test <- titanic[-tmp,]

``` 
<report text for task 1 comes here> 
  
## Task 2 
```{r} 
#your code for task 2 comes here 
train[train==""]<-NA
test[test==""]<-NA
``` 
<report text for task 2 comes here> 
The count of missing data for the training data is:
```{r} 
na_train <-apply(train, 2, function(y) sum(is.na(y)))
print(na_train)
```
The count of missing data for the testing data is:
```{r} 
na_test <-apply(test, 2, function(y) sum(is.na(y)))
print(na_test)
```

## Task 3 
```{r} 

``` 
<report text for task 3 comes here> 
The attributes that I will use as features are pclass, age, sex, sibsp, parch, fare, ticket, cabin, embarked, name and home.dest as all of the above attributes are past data. 

Boat and body are not used as they are future data, we can only obtain the data for these two attributes after the titanic hit the iceberg.

## Task 4 
```{r} 
preprocess <- function(name_frame){
  mean_age <- mean(name_frame[,5], na.rm=TRUE)
  mean_fare <- mean(name_frame[,9], na.rm=TRUE)
  count(name_frame,"embarked")
  name_frame$embarked[is.na(name_frame[,11])] <- "S"
  name_frame$age[is.na(name_frame[,5])] <- mean_age
  name_frame$fare[is.na(name_frame[,9])] <- mean_fare
  name_frame<- name_frame[,-c(3,8,10,12:14)]
  name_frame<- mutate(name_frame,
    pclass = factor(pclass, levels = c(1,2,3), labels = c("Top", "Middle", "Basic")),
    survived = factor(survived, levels = c(0,1), labels = c(FALSE, TRUE))
  )
  return(name_frame)
}

train <- preprocess(train)
test <- preprocess(test) 

``` 
<report text for task 4 comes here> 
I have dropped the columns: cabin, boat, body, home.dest and the reasons are as follows:
Count of missing data from the original data set:
```{r} 
titanic[titanic==""]<-NA
na_count <-apply(titanic, 2, function(y) sum(is.na(y)))
print(na_count)
``` 
cabin, boat, body:
all 3 of the columns have more than 60% of data missing and the classification based on these attributes will not be accurate. Even if we try to replace missing data with estimate values, the value that get replaced will not be accurate or useful as we dont have a good estimate for the data. Therefore, these three columns are dropped.

home.dest:
The reason for dropping the home.dest column is that the data is too spread out as most of the passengers have different home address, if we include this feature in the decision tree or use it as a predicting feature, there is a high chance such that we will not have an accurate estimation. Therefore, the home.dest is dropped even when the column has around 60% of useable data.

ticket, name:
Although we have almost complete data on attribute name and column, the data is unique and hence too spread out and with too many levels. Moreover, the two attributes may not contribute heavily when estimating the survival rate of the passenger. Therefore, these 2 columns are dropped.

fare, age:
For the columns fare and age, the missing data is being filled with the mean value of the respective column. Filling the missing data with the column mean should not affect the overall mining result and we can still obtain an accurate estimation on the passengers survival rate with these two columns included as featuers.

embarked:
For the embarked column, the missing data is being filled with the data of highest frequency of the column. Since the values in the embarked column is categorical, we cannot take the mean like age and fare to replace the missing value. Using the mode of the column to fill up the missing data should not affect the overall distribution of data and we can still obtain a good estimation of the data we want to investigate.

pclass:
for the column pclass, I mutated the value from (1,2,3) to (Top,Middle,Basic). By changing the represenation method of the data, it will be easier to observe in the tree as they have become categorical data.

survived:
for the column pclass, I mutated the value from (0,1) to booleans (FALSE, TRUE). By changing the represenation method of the data, it will be easier to observe in the tree as they have become boolean data.

## Task 5 
```{r} 
train_tree <- tree(formula = survived ~., data = train, method="class")
plot(train_tree, type="uniform")
text(train_tree, all=TRUE, pretty=0)
``` 
<report text for task 5 comes here> 
```{r} 
summary(train_tree)
``` 
The size of the is 7.
```{r} 
survived.pred <- predict(train_tree, test, type="class")
table_pred <- table(test$survived, survived.pred)
table_pred
accuracy <- sum(diag(table_pred)) / sum(table_pred)
print(accuracy)
``` 
The accuracy of the tree on the test dataset equals to 0.8206107.

## Task 6 
```{r} 
plot(train_tree, type="uniform")
text(train_tree, all=TRUE, pretty=0)
``` 
<report text for task 6 comes here> 
\n From the decision tree, we can conclude that the three most important features are sex, pclass and age in decreasing order. The more closer for an attribute to be used to split data near the root, it means the more weight that attribute carries in terms of predicting the classification of the data.Sex is used for the first split of from the root, hence it is the most important attribute among all. pclass is used afterwards hence it is the second most important and followed by age as the third.

When explaining without data mining knowledge, sex is the most important factor becuase generally females are prioritized to be saved before male in commmon social practic. The males are exposed to higher risk when getting all the females to safety first before getting themselves out of danger. For pclass, it is important as it may be possible such that higher ordinal value of pclass means closer to the escape boat, hence higher chance of survival. And for age, the children and elderly are prioritized to be saved first, hence having higher chance of survival. The decision tree supports such idea as it uses 9.5 as the best spliting point when spliting the attribute age.

From the decision tree, we learn that a male who belongs to top/middle pclass, who belongs to normal pclass and purchase a ticket with less than $23.35 or a female who ages under 9.5 and have less than 2.5 of sibsp have high chance of survival.

## Task 7 
```{r} 
train.cv <- cv.tree(train_tree)
optimial_k <- train.cv$size[which.min(train.cv$dev)]
print(optimial_k)
``` 
<report text for task 7 comes here> 
By using the function cv.tree(), we calculate the deviance or number of misclassifications as a function of the cost-complexity parameter by running a K-fold cross-validation experiment. And cv.tree function returned the optimal level of tree complexity to be 7.
```{r} 
prune_tree <- prune.tree(train_tree, best=optimial_k)
summary(prune_tree)
``` 
According to the reading the summary of the pruned tree, the size of the pruned decision tree is 
7.